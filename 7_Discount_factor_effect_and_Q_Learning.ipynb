{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-nudo/intelligent-tools/blob/master/7_Discount_factor_effect_and_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Discount factor effect and Q-Learning"
      ],
      "metadata": {
        "id": "Uj3ZwlkRQKpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Based on the probabilities on the arrows above, Model your MDP (transition probabilities and rewards) in the notebook. you can use the [s, a, sâ€™] for each part or you can use your own way of defining the MDP. What should be seen are transition probabilities, rewards and possible actions"
      ],
      "metadata": {
        "id": "JUL_pNnRBOb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transition_probabilities = {\n",
        "    ('s0', 'a0', 's0'): 0.7,\n",
        "    ('s0', 'a0', 's1'): 0.3,\n",
        "    ('s0', 'a1', 's0'): 1.0,\n",
        "    ('s0', 'a2', 's1'): 0.2,\n",
        "    ('s0', 'a2', 's0'): 0.8,\n",
        "    ('s1', 'a0', 's1'): 1.0,\n",
        "    ('s1', 'a2', 's2'): 1.0,\n",
        "    ('s2', 'a1', 's0'): 0.8,\n",
        "    ('s2', 'a1', 's1'): 0.1,\n",
        "    ('s2', 'a1', 's2'): 0.1,\n",
        "}\n",
        "\n",
        "rewards = {\n",
        "    ('s0', 'a0', 's0'): 10,\n",
        "    ('s1', 'a2', 's2'): 50,\n",
        "    ('s2', 'a1', 's0'): 40,\n",
        "}\n",
        "\n",
        "possible_actions = {\n",
        "    's0': ['a0', 'a1', 'a2'],\n",
        "    's1': ['a0', 'a2'],\n",
        "    's2': ['a1'],\n",
        "}"
      ],
      "metadata": {
        "id": "gQ8nWasjQK-x"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "JkFqkJCuQPOi"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values = Q_values2 = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "# Q_values2 = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
        "for state, actions in possible_actions.items():\n",
        "#     print(state, actions)\n",
        "#     Q_values[state, actions] = 0.0  # for all possible actions\n",
        "  for action in actions:\n",
        "    action_index = ord(action[1]) - ord('0') # Convert action to index\n",
        "    Q_values[int(state[1]), action_index] = 0.0  # for all possible actions\n",
        "    Q_values2[int(state[1]), action_index] = 0.0"
      ],
      "metadata": {
        "id": "-mcUQqriQLoW"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2s-PAu6CSXE",
        "outputId": "13af6c1c-42eb-45e8-8b5c-9d5d8f4f2625"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.,   0.,   0.],\n",
              "       [  0., -inf,   0.],\n",
              "       [-inf,   0., -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhryOaZDCUUV",
        "outputId": "dba998a0-bf87-4aac-c6f0-3206f9dc70c5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0.,   0.,   0.],\n",
              "       [  0., -inf,   0.],\n",
              "       [-inf,   0., -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Take your discount factor to be 0.9. Perform Q-learning and report the Q-values for each (state, action) pair. Based on that, what is the optimal policy?"
      ],
      "metadata": {
        "id": "y1aW8WF16GOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_learning(Q_values, gamma, alpha):\n",
        "  for iteration in range(50):\n",
        "    # --- fill here (perform a DP approach for filling up your Q-table (repeat the process by stting gamma to be 0.95 )\n",
        "    for state in possible_actions.keys():\n",
        "      for action in possible_actions[state]:\n",
        "          action_index = ord(action[1]) - ord('0')\n",
        "          next_states = [s for (s, a, s_) in transition_probabilities.keys() if s == state and a == action]\n",
        "          probabilities = [transition_probabilities[(s, a, s_)] for (s, a, s_) in transition_probabilities.keys() if s == state and a == action]\n",
        "          next_state = random.choices(next_states, probabilities)[0]\n",
        "          reward = rewards.get((state, action, next_state), 0)\n",
        "          max_q_next_state = np.max(Q_values[int(next_state[1]), :])\n",
        "          Q_values[int(state[1]), action_index] = (1 - alpha) * Q_values[int(state[1]), action_index] + alpha * (reward + gamma * max_q_next_state)\n",
        "\n",
        "gamma = 0.9  # the discount factor\n",
        "alpha = 0.1  # the learning rate\n",
        "q_learning(Q_values, gamma, alpha)"
      ],
      "metadata": {
        "id": "zx3UhYrBQNiu"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values"
      ],
      "metadata": {
        "id": "gdRbvTjMQTWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a56cd6-2e03-4f5f-ce03-9eec12402ea3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[39.49939329, 30.15078333, 30.15078333],\n",
              "       [ 0.        ,        -inf,  0.        ],\n",
              "       [       -inf,  0.        ,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values.argmax(axis=1)  # optimal action for each state"
      ],
      "metadata": {
        "id": "lGHSyhS3QV_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b4eb90-fa7f-4f44-cb90-516931bd1737"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy = {}\n",
        "for state in possible_actions.keys():\n",
        "    action_index = np.argmax(Q_values[int(state[1]), :])\n",
        "    # action_index = ord(action[1]) - ord('0')\n",
        "    action_mapping = {\n",
        "        'a0': 0,\n",
        "        'a1': 1,\n",
        "        'a2': 2,\n",
        "    }\n",
        "    optimal_action = [a for a, i in action_mapping.items() if i == action_index][0]\n",
        "    optimal_policy[state] = optimal_action\n",
        "\n",
        "print(\"Optimal policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: {action}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9kz0GJDDQyo",
        "outputId": "c17056b8-c104-4c60-e8b6-b06f5b68ce44"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy:\n",
            "State s0: a0\n",
            "State s1: a0\n",
            "State s2: a1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Perform the same procedure but this time with a discount factor of 0.95. Did your optimal policy change? Explain your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "cyS6xw6MCu8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gamma2 = 0.95  # the discount factor\n",
        "alpha2 = 0.1  # the learning rate\n",
        "q_learning(Q_values2, gamma2, alpha)"
      ],
      "metadata": {
        "id": "0xwcXCskC0aD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQJKW29IBlLs",
        "outputId": "5a76d56f-6440-4100-eb49-cfea4660efc7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[44.33748859, 35.16218512, 35.16218512],\n",
              "       [ 0.        ,        -inf,  0.        ],\n",
              "       [       -inf,  0.        ,        -inf]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_values2.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03AUnycLBmzj",
        "outputId": "062f3169-ea41-4248-a356-7e102278fa83"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy = {}\n",
        "for state in possible_actions.keys():\n",
        "    action_index = np.argmax(Q_values2[int(state[1]), :])\n",
        "    # action_index = ord(action[1]) - ord('0')\n",
        "    action_mapping = {\n",
        "        'a0': 0,\n",
        "        'a1': 1,\n",
        "        'a2': 2,\n",
        "    }\n",
        "    optimal_action = [a for a, i in action_mapping.items() if i == action_index][0]\n",
        "    optimal_policy[state] = optimal_action\n",
        "\n",
        "print(\"Optimal policy:\")\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: {action}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA0hS_lRE6WH",
        "outputId": "8ee93ff9-7efe-4ab4-f650-002bf077c6a3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy:\n",
            "State s0: a0\n",
            "State s1: a0\n",
            "State s2: a1\n"
          ]
        }
      ]
    }
  ]
}