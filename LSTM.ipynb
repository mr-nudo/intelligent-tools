{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-nudo/intelligent-tools/blob/master/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Import the required libraries"
      ],
      "metadata": {
        "id": "-ShLoND4kvBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "JNm5tMT4owV2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding random seed"
      ],
      "metadata": {
        "id": "Pc0lsInBt0hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "os.environ['PYTHONHASHSEED'] = str(25)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "# Set seed values\n",
        "np.random.seed(25)\n",
        "tf.random.set_seed(25)\n",
        "random.seed(25)"
      ],
      "metadata": {
        "id": "-fG-gUjRt9N-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Read and Preprocess the dataset"
      ],
      "metadata": {
        "id": "PA28CYq9kzYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/alice.txt'\n",
        "text = \"\"\n",
        "\n",
        "# Load and preprocess the text\n",
        "with open(path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "lines = text.split('\\n')"
      ],
      "metadata": {
        "id": "UF9gYFvBk4zQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text))"
      ],
      "metadata": {
        "id": "2UHmLguZRMUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4f716d-411f-4729-ebce-dcc3d845b72f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Using tokenizers"
      ],
      "metadata": {
        "id": "76FWfPQnlCfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Tokenize the text\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1"
      ],
      "metadata": {
        "id": "g8DkbTcZlDe6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_words)"
      ],
      "metadata": {
        "id": "66GlWnQmRPcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef91813b-4228-4665-cb32-6b8afece069a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Feature Engineering"
      ],
      "metadata": {
        "id": "9f7lULFglL2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create input sequences\n",
        "input_sequences = []\n",
        "\n",
        "for line in lines:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokens) + 1):\n",
        "        input_sequences.append(tokens[:i])\n",
        "\n",
        "# Identify the maximum sequence length\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "\n",
        "# Pad sequences\n",
        "padded_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Print the number of input sequences\n",
        "# num_sequences = len(padded_sequences)\n",
        "# print(num_sequences)\n",
        "\n",
        "# Optionally, print the padded sequences and tokenizer word index\n",
        "# print(padded_sequences)\n",
        "# print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "szpzGGFilQQF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_sequences))"
      ],
      "metadata": {
        "id": "xznaaN41YXsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb090fca-fdda-493a-d5ff-1a47f7c8ad33"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Storing features and labels"
      ],
      "metadata": {
        "id": "3Sx1nF7ClSwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create predictors and labels\n",
        "predictors = padded_sequences[:, :-1]\n",
        "labels = padded_sequences[:, -1]\n",
        "\n",
        "# One-hot encode the labels\n",
        "labels = to_categorical(labels, num_classes=len(tokenizer.word_index) + 1)\n",
        "\n",
        "# TODO: Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(predictors, labels, test_size=0.2, random_state=25)\n",
        "\n",
        "# Print the sizes of the train and validation subsets\n",
        "print(\"Training features size:\", X_train.shape)\n",
        "print(\"Validation features size:\", X_val.shape)\n",
        "print(\"Training labels size:\", y_train.shape)\n",
        "print(\"Validation labels size:\", y_val.shape)"
      ],
      "metadata": {
        "id": "U72XiieKlTAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9706929-6729-4d6f-bbc3-3c302379aa2a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features size: (21128, 15)\n",
            "Validation features size: (5282, 15)\n",
            "Training labels size: (21128, 2751)\n",
            "Validation labels size: (5282, 2751)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Building our model"
      ],
      "metadata": {
        "id": "J2D4YaQ2lWxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Build your model"
      ],
      "metadata": {
        "id": "x1-88fIblY1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Model training"
      ],
      "metadata": {
        "id": "mgqvWzdUleUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train your model"
      ],
      "metadata": {
        "id": "vhx4p1lWldOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Visualising the Training and Validation Accuracies and Losses against the number of Epochs"
      ],
      "metadata": {
        "id": "69szYwGslknP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plotting the training and validation loss and accuracy\n"
      ],
      "metadata": {
        "id": "ti0KtAsJllVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Generate text"
      ],
      "metadata": {
        "id": "qMsd889wuvR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Function to generate text\n",
        "def generate_text():\n",
        "    # TODO\n",
        "\n",
        "# Generate text\n",
        "seed_text = \"Forest is\"\n",
        "next_words = 10\n",
        "generated_text = generate_text()\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "SXHE7cU_uxRx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}