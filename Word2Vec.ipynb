{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-nudo/intelligent-tools/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "gqeGGFcnokoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "31x1tVfgdmZq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Lambda, Average\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set random seed"
      ],
      "metadata": {
        "id": "O2FiqCLjvCbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "os.environ['PYTHONHASHSEED'] = str(25)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "# Set seed values\n",
        "np.random.seed(25)\n",
        "tf.random.set_seed(25)\n",
        "random.seed(25)"
      ],
      "metadata": {
        "id": "ziEj8ly6vE9r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Preprocess"
      ],
      "metadata": {
        "id": "9tT4SznWvOMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def preprocess(text):\n",
        "    lowercase_text = text.lower()\n",
        "    words = lowercase_text.split()\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "fuuTn2EzvREK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Build Vocabulary and training data"
      ],
      "metadata": {
        "id": "0Wi3Pk-5vVd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary and generate training data\n",
        "def build_and_prepare_data(words, window_size):\n",
        "    # Build vocabulary\n",
        "    vocab = set(words)\n",
        "    vocab = list(vocab)\n",
        "    vocab_size = len(vocab)\n",
        "    word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "\n",
        "    # Generate context-target pairs\n",
        "    contexts = []\n",
        "    targets = []\n",
        "    for i in range(window_size, len(words) - window_size):\n",
        "        context = words[i - window_size:i] + words[i + 1:i + window_size + 1]\n",
        "        target = words[i]\n",
        "        contexts.append(context)\n",
        "        targets.append(target)\n",
        "\n",
        "    # Extract contexts and targets from data\n",
        "    contexts = [word_to_index[word] for word in contexts]\n",
        "    targets = [word_to_index[target] for target in targets]\n",
        "\n",
        "    # Prepare contexts and targets for training by padding and one-hot encoding\n",
        "    max_context_len = max(len(context) for context in contexts)\n",
        "    contexts = pad_sequences(contexts, maxlen=max_context_len, padding='post')\n",
        "    targets = to_categorical(targets, num_classes=vocab_size)\n",
        "\n",
        "    return vocab, contexts, targets"
      ],
      "metadata": {
        "id": "b1oDT32ovXyG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Build CBOW model"
      ],
      "metadata": {
        "id": "EZzb-mKEvjt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CBOW model function\n",
        "def build_cbow_model(vocab_size, embed_size, window_size):\n",
        "    # Define the model architecture\n",
        "    inputs = Input(shape=(window_size,))\n",
        "\n",
        "    # Embedding layer to convert words into vectors\n",
        "    embeddings = Embedding(vocab_size, embed_size)(inputs)\n",
        "\n",
        "    # Average layer to combine context word vectors\n",
        "    average = Average()(embeddings)\n",
        "\n",
        "    # Output layer with softmax for predicting target word\n",
        "    outputs = Dense(vocab_size, activation='softmax')(average)\n",
        "\n",
        "    # Compiling model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DR_mXHgbvltf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Set file path"
      ],
      "metadata": {
        "id": "0Oq50BtE1lWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: set correct file path\n",
        "file_path = 'path_to_small_corpus.txt'"
      ],
      "metadata": {
        "id": "7fek7pgq1oSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the helper functions"
      ],
      "metadata": {
        "id": "_pHa3gGV1tIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "words = preprocess(text)\n",
        "\n",
        "# Print vocabulary size\n",
        "print(f\"Number of words: {len(words)}\")\n",
        "\n",
        "# Model parameters\n",
        "window_size = 2\n",
        "\n",
        "# Prepare dataset\n",
        "vocab, contexts, targets = build_and_prepare_data(words, window_size)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "# Print vocabulary size\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Print lengths of contexts and targets\n",
        "print(f\"Length of contexts array: {len(contexts)}\")\n",
        "print(f\"Length of targets array: {len(targets)}\")"
      ],
      "metadata": {
        "id": "h1VnZMEUvqW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data inton training and validation sets"
      ],
      "metadata": {
        "id": "JAeFt_mZwejS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data\n",
        "contexts_train, contexts_val, targets_train, targets_val = train_test_split(contexts, targets, test_size=0.2, random_state=25)\n",
        "\n",
        "embed_size = 2"
      ],
      "metadata": {
        "id": "EKaUyZlSvzU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "7w_SJZcawmFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the model\n",
        "model = build_cbow_model(vocab_size, embed_size, window_size)\n",
        "history = model.fit(contexts_train, targets_train, validation_data=(contexts_val, targets_val), epochs=7, verbose=1)\n"
      ],
      "metadata": {
        "id": "M8SK57mFwa0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Visualise the Training and Validation loss"
      ],
      "metadata": {
        "id": "ARDpEQEYwqeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation loss\n",
        "#TODO"
      ],
      "metadata": {
        "id": "knS04YQgwvBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Extract the embeddings"
      ],
      "metadata": {
        "id": "uARqDG-twyV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings\n",
        "#TODO"
      ],
      "metadata": {
        "id": "G2cN8QXMnc7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Find similar words"
      ],
      "metadata": {
        "id": "5AuaC0wlw5AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
        "    #TODO\n",
        "    return similarity\n",
        "\n",
        "def find_similar_words(query_word, vocab, embeddings, top_n=3):\n",
        "    \"\"\"Find the top_n words most similar to the query_word based on the embeddings.\"\"\"\n",
        "    similarities = []\n",
        "\n",
        "    #TODO populate the similarities list\n",
        "\n",
        "\n",
        "\n",
        "    # Sort based on similarity scores\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Print top similar words\n",
        "    print(f\"Words most similar to '{query_word}':\")\n",
        "    for word, similarity in similarities[:top_n]:\n",
        "        print(f\"{word}: {similarity:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0wt6O91ajECC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_words = ['poland', 'thailand', 'morocco']\n",
        "\n",
        "for query_word in query_words:\n",
        "    find_similar_words(query_word, vocab, embeddings)\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "Ezzy52UyjFR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: Visualise the embeddings"
      ],
      "metadata": {
        "id": "M4eJgZRQxFz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of the embeddings\n",
        "# TODO"
      ],
      "metadata": {
        "id": "vB3ZFrgapYr_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}