{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr-nudo/intelligent-tools/blob/master/Gradient_descent_for_softmax_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M_OGYVszbf_o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using only NumPy, implement batch gradient descent for softmax regression. Train this classifier on the Iris dataset, demonstrating the process without relying on Scikit-learn."
      ],
      "metadata": {
        "id": "boJoyGKU69AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the Iris data as provided in the notebook of the assignment. Add the bias term for every instance (x0 = 1)."
      ],
      "metadata": {
        "id": "FzL2OUW07UQn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z2Ek6YxS66_J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d48879d-696f-4c6b-a05f-1ede06b7a361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "iris = load_iris(as_frame=True)\n",
        "print(iris.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the data (take the petal length and petal width)."
      ],
      "metadata": {
        "id": "Kn-xNO-DSzxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.data[['petal length (cm)', 'petal width (cm)']].values   # (take the petal length and petal width)\n",
        "y = iris.target.values"
      ],
      "metadata": {
        "id": "EZMEiCzXSw8P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add the bias term for every instance ($x_0 = 1$)."
      ],
      "metadata": {
        "id": "4RnnMFjJS5_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_with_bias = np.c_[np.ones(X.shape[0]), X] #----- fill here"
      ],
      "metadata": {
        "id": "ZW9qx4OvS3qO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the dataset into a training set, a validation set and a test set manually:"
      ],
      "metadata": {
        "id": "NwIHFNA4S_w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "np.random.seed(42)\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "TkXszuKJTDDC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. The targets are class indices (0, 1 or 2). They have to turn to class probabilities to train the Softmax Regression model. Each instance must show a probability equal to 0.0 for all classes except for the target class (1.0) (so the class probability vectors should be a one-hot vector). Write a function to convert the vector of class indices to a matrix of one-hot vector for each instance.\n"
      ],
      "metadata": {
        "id": "OVoHR9fmPqPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(y):\n",
        "    y = np.array(y).reshape(-1)\n",
        "    one_hot = np.eye(3)[y]\n",
        "    return one_hot\n",
        ""
      ],
      "metadata": {
        "id": "n2HnyBuvPtuX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check with the expected out put to make sure your code is doing the right thing:"
      ],
      "metadata": {
        "id": "zjBHzjmnTI7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:10]"
      ],
      "metadata": {
        "id": "A7Pe-g_ZTGd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0444165-9a61-48f1-cc06-20113a990ddd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_one_hot(y_train[:10])"
      ],
      "metadata": {
        "id": "61hTgdCBTbQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967d00b5-dc84-47fa-b506-d75279c32837"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "6skDbxxLThoz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Normalize the data using Z-Score Normalization and define the softmax function to be used later."
      ],
      "metadata": {
        "id": "QdAtbXKdP0-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill the following lines\n",
        "mean = np.mean(X_train[:, 1:], axis=0)\n",
        "std = np.std(X_train[:, 1:], axis=0)\n",
        "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
        "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
        "X_test[:, 1:] = (X_test[:, 1:] - mean) / std\n"
      ],
      "metadata": {
        "id": "YejzegQbP4UO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "metadata": {
        "id": "8T1Zx55gTlw4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = X_train.shape[1]  # == 3 (2 features plus the bias term)\n",
        "n_outputs = len(np.unique(y_train))  # == 3 (there are 3 iris classes)"
      ],
      "metadata": {
        "id": "ng7_oCt4TnE2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs"
      ],
      "metadata": {
        "id": "OnycF8qtTp0t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff924eb-c220-49f3-f176-bec0fcd655f8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Implement the gradient step using numpy. Make sure about the dimensions and the correctness of your calculations."
      ],
      "metadata": {
        "id": "KKnDPXUuTsFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "turn  the math equations into Python code.\n",
        "\n",
        "So the equations we will need are the cost function:\n",
        "\n",
        "$J(\\mathbf{\\Theta}) =\n",
        "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
        "\n",
        "And the equation for the gradients:\n",
        "\n",
        "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
        "\n",
        "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values."
      ],
      "metadata": {
        "id": "_JGQZtgAT2qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.5\n",
        "n_epochs = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-5\n",
        "\n",
        "np.random.seed(42)\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "\n",
        "# fill the following lines:\n",
        "for epoch in range(n_epochs):\n",
        "    logits = #----\n",
        "    Y_proba = #----\n",
        "    if epoch % 1000 == 0:\n",
        "        Y_proba_valid = softmax#(----)\n",
        "        xentropy_losses = #-----\n",
        "        print(epoch, xentropy_losses.sum(axis=1).mean())\n",
        "    error = #-----\n",
        "    gradients = # -----\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "id": "KPHWVMFLTwON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Theta"
      ],
      "metadata": {
        "id": "GRxNZ_XdT9nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid @ Theta\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = Y_proba.argmax(axis=1)\n",
        "\n",
        "accuracy_score = (y_predict == y_valid).mean()\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "WR7z9chiUAVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Document and plot the results of your model."
      ],
      "metadata": {
        "id": "wJsIRLXgT6Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "custom_cmap = mpl.colors.ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
        "\n",
        "x0, x1 = np.meshgrid(np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "                     np.linspace(0, 3.5, 200).reshape(-1, 1))\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "X_new = (X_new - mean) / std\n",
        "X_new_with_bias = np.c_[np.ones(len(X_new)), X_new]\n",
        "\n",
        "logits = #---- fill here\n",
        "Y_proba = #---- fill here\n",
        "y_predict = #---- fill here\n",
        "\n",
        "zz1 = Y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y == 2, 0], X[y == 2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=\"hot\")\n",
        "plt.clabel(contour, inline=1)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYPxVZIUUEn7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}